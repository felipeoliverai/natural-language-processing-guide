{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmZmpOtgriS_"
      },
      "source": [
        "### Tokenization \r\n",
        "\r\n",
        "\r\n",
        "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSJvYFBqC4B",
        "outputId": "aa41c280-e017-444e-ccc8-d5d7e4cc10b8"
      },
      "source": [
        "import nltk \r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "from nltk.tokenize import TreebankWordTokenizer\r\n",
        "from nltk.tokenize import WordPunctTokenizer\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "from nltk.tokenize import regexp_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kSHwmQDqHNz"
      },
      "source": [
        "text = \"This is my sentence. that will be tokenize.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVrD3L6AqaBI",
        "outputId": "4816a244-47c6-4954-a9c2-cae4ffd3ddf1"
      },
      "source": [
        "# word tokenization \r\n",
        "word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'my', 'sentence', '.', 'that', 'will', 'be', 'tokenize', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRcaZ6F6qb9E",
        "outputId": "c4f5925e-04d6-4500-b1e4-a6f683822ea9"
      },
      "source": [
        "# sentence tokenization \r\n",
        "sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is my sentence.', 'that will be tokenize.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PorRT3V0qlGN",
        "outputId": "ced20fcb-1073-4104-f1bd-35b91645454c"
      },
      "source": [
        "len(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9U_1eyEt3S_",
        "outputId": "7e77942f-89e3-4861-b094-8ee450146148"
      },
      "source": [
        "# contraction error tokenizer \r\n",
        "print(word_tokenize(\"can't\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ca', \"n't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Rr6ZlPrOt-"
      },
      "source": [
        "<br>\r\n",
        "<hr>\r\n",
        "<br>\r\n",
        "\r\n",
        "\r\n",
        "#### Custom Tokenization \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank. This is the method that is invoked by word_tokenize(). It assumes that the text has already been segmented into sentences, e.g. using sent_tokenize().\r\n",
        "\r\n",
        "This tokenizer performs the following steps:\r\n",
        "\r\n",
        "split standard contractions, e.g. don't -> do n't and they'll -> they 'll\r\n",
        "\r\n",
        "treat most punctuation characters as separate tokens\r\n",
        "\r\n",
        "split off commas and single quotes, when followed by whitespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qpeaEreqp7e",
        "outputId": "1eb7942a-f69b-41ba-8e5a-1344673136ab"
      },
      "source": [
        "# custom tokenization \r\n",
        "tokenizer = TreebankWordTokenizer()\r\n",
        "print(tokenizer.tokenize('Hello World.'))\r\n",
        "print(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'World', '.']\n",
            "['This', 'is', 'my', 'sentence.', 'that', 'will', 'be', 'tokenize', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_p6eOLcrSU9"
      },
      "source": [
        "<br>\r\n",
        "<hr>\r\n",
        "<br>\r\n",
        "\r\n",
        "\r\n",
        "#### Punctuation tokenization \r\n",
        "\r\n",
        "An alternative word tokenizer that splits all punctuation into separate tokens.\r\n",
        "\r\n",
        "\r\n",
        "Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \\w+|[^\\w\\s]+."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2TFDMasrJcD",
        "outputId": "33b0228b-7d26-4a32-b804-83c63e201099"
      },
      "source": [
        "tokenizer = WordPunctTokenizer()\r\n",
        "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYBE0pm6rmGP"
      },
      "source": [
        "<br>\r\n",
        "<hr>\r\n",
        "<br>\r\n",
        "\r\n",
        "#### Regex Tokenization \r\n",
        "\r\n",
        "how to tokenize the text, we have regular expression which can be used while doing sentence tokenization. NLTK provide RegexpTokenizer class to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqG6GLJlrqli",
        "outputId": "93da215d-d66d-4e7a-bd8b-71126332766f"
      },
      "source": [
        "# Regex tokenization \r\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\r\n",
        "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Can't\", 'is', 'a', 'contraction']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8YQ8ZW_rste",
        "outputId": "75ecab41-f4bc-466c-8502-b888677c583c"
      },
      "source": [
        "print(regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Can't\", 'is', 'a', 'contraction']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_buHPU1Kr8jo",
        "outputId": "76c949b4-b07c-47f8-84b7-711ae5ce503c"
      },
      "source": [
        "tokenizer = RegexpTokenizer('\\s+', gaps = True)\r\n",
        "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Can't\", 'is', 'a', 'contraction.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}